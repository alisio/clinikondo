# üìã Changelog - Sistema de Detec√ß√£o de Duplicatas

**Data**: 17 de Outubro de 2025  
**Vers√£o**: 2.1 - Gest√£o de Duplicatas e Reprocessamento  
**Baseado em**: SRS v2.0 - Se√ß√£o 6 (Gest√£o de Duplicatas e Reprocessamento)

## üéØ Objetivo

Implementar sistema completo de detec√ß√£o de duplicatas conforme especificado na **Se√ß√£o 6** do documento de requisitos, incluindo:

1. ‚úÖ Duplicata por Hash ‚Üí Pular com log informativo (evitar custos LLM)
2. ‚úÖ Nome Duplicado ‚Üí Vers√£o numerada autom√°tica (`_v2`, `_v3`, etc.)
3. ‚úÖ Flag `--force-reprocess` ‚Üí Permitir reprocessamento intencional
4. ‚úÖ Logs Audit√°veis ‚Üí Registrar todas as decis√µes de duplicata

---

## üì¶ Arquivos Modificados

### 1. `src/clinikondo/config.py`

**Altera√ß√µes**:
- ‚úÖ Adicionado campo `force_reprocess: bool = False` ao dataclass `Config`
- ‚úÖ Adicionado propriedade `processed_hashes_path` para arquivo de rastreamento
- ‚úÖ Carregamento de `force_reprocess` via CLI args e vari√°vel de ambiente `CLINIKONDO_FORCE_REPROCESS`

**Justificativa**: Permite ao usu√°rio for√ßar reprocessamento de documentos ignorando cache de hashes (√∫til ap√≥s melhorias no prompt ou mudan√ßa de modelo).

---

### 2. `src/clinikondo/__main__.py`

**Altera√ß√µes**:
- ‚úÖ Adicionado argumento CLI `--force-reprocess` ao parser do comando `processar`

**Uso**:
```bash
python -m src.clinikondo processar \
  --input ~/clinikondo/entrada \
  --output ~/clinikondo/saida \
  --model <modelo> \
  --force-reprocess  # ‚Üê Nova flag
```

**Justificativa**: Interface CLI para flag de reprocessamento conforme SRS 6.0.

---

### 3. `src/clinikondo/hash_tracker.py` ‚≠ê **NOVO ARQUIVO**

**Funcionalidades Implementadas**:

#### Classe `ProcessedFileRecord`
```python
@dataclass
class ProcessedFileRecord:
    hash_sha256: str
    arquivo_original: str
    arquivo_destino: str
    timestamp: str
    paciente_slug: str
    tipo_documento: str
```

#### Classe `HashTracker`
- ‚úÖ `calculate_hash(file_path)` - Calcula SHA-256 de arquivos
- ‚úÖ `is_processed(file_hash)` - Verifica se hash j√° foi processado
- ‚úÖ `get_record(file_hash)` - Obt√©m registro de arquivo processado
- ‚úÖ `add_record(...)` - Adiciona novo registro ao cache
- ‚úÖ `log_duplicate_detection(...)` - Gera logs audit√°veis em JSON
- ‚úÖ `get_statistics()` - Estat√≠sticas de processamento
- ‚úÖ Persist√™ncia autom√°tica em `<output>/.clinikondo/processed_hashes.json`

**Formato do Log Audit√°vel** (SRS 6.0):
```json
{
  "evento": "duplicata_detectada",
  "tipo_duplicata": "hash_identico",
  "arquivo_novo": "/path/to/novo_arquivo.pdf",
  "arquivo_original": "/path/to/arquivo_original.pdf",
  "hash_sha256": "abc123...",
  "acao": "processamento_pulado",
  "custo_economizado": "1_chamada_llm",
  "timestamp": "2025-10-17T10:30:00Z"
}
```

---

### 4. `src/clinikondo/processing.py`

**Altera√ß√µes**:

#### a) Importa√ß√£o e Inicializa√ß√£o
```python
from .hash_tracker import HashTracker

class DocumentProcessor:
    def __init__(...):
        # ...
        self.hash_tracker = HashTracker(config.processed_hashes_path)
```

#### b) Detec√ß√£o de Duplicatas por Hash (m√©todo `process_all`)
```python
# Verificar duplicata por hash (SRS 6.0 - Detec√ß√£o de Duplicatas)
if not self.config.force_reprocess:
    file_hash = self.hash_tracker.calculate_hash(path)
    if self.hash_tracker.is_processed(file_hash):
        existing_record = self.hash_tracker.get_record(file_hash)
        self.hash_tracker.log_duplicate_detection(
            file_hash=file_hash,
            arquivo_novo=str(path),
            arquivo_original=existing_record.arquivo_original,
            tipo_duplicata="hash_identico",
            acao="processamento_pulado",
            custo_economizado="1_chamada_llm"
        )
        LOGGER.info(f"‚è≠Ô∏è  Arquivo duplicado detectado (hash: {file_hash[:12]}...) - pulando processamento")
        skipped_duplicates += 1
        continue
```

**Comportamento**:
- ‚úÖ Calcula hash SHA-256 antes de processar
- ‚úÖ Se hash j√° existe no cache ‚Üí **PULA processamento** (economiza chamada LLM)
- ‚úÖ Registra log audit√°vel com informa√ß√µes completas
- ‚úÖ Mostra estat√≠sticas de duplicatas puladas ao final

#### c) Registro de Hash Processado (m√©todo `_process_single`)
```python
# Calcular hash do arquivo (SRS 6.0 - Detec√ß√£o de Duplicatas)
file_hash = self.hash_tracker.calculate_hash(path)
document.hash_sha256 = file_hash

# ... processamento ...

if not self.config.dry_run:
    self._move_document(document.caminho_entrada, destination_path)
    
    # Registrar hash processado (SRS 6.0 - Rastreamento de Hashes)
    self.hash_tracker.add_record(
        file_hash=file_hash,
        arquivo_original=str(path),
        arquivo_destino=str(destination_path),
        paciente_slug=patient.slug_diretorio,
        tipo_documento=document.tipo_documento
    )
```

#### d) Versionamento Num√©rico para Nomes Duplicados (m√©todo `_unique_destination`)
```python
def _unique_destination(self, target: Path) -> Path:
    """Gera nome √∫nico para arquivo, usando vers√£o numerada se necess√°rio (SRS 6.0).
    
    Formato: nome-arquivo_v2.ext, nome-arquivo_v3.ext, etc.
    """
    if not target.exists():
        return target
    stem = target.stem
    suffix = target.suffix
    counter = 2  # Come√ßar em _v2
    while True:
        candidate = target.with_name(f"{stem}_v{counter}{suffix}")
        if not candidate.exists():
            return candidate
        counter += 1
```

**Mudan√ßa**: Alterado de `{stem}-{counter}` para `{stem}_v{counter}` conforme SRS.

#### e) Log de Nomes Duplicados (m√©todo `_process_single`)
```python
# Se nome foi alterado (duplicado), registrar no log (SRS 6.0 - Nome Duplicado)
if destination_path.name != final_name:
    self.hash_tracker.log_duplicate_detection(
        file_hash=file_hash,
        arquivo_novo=str(path),
        arquivo_original="N/A",
        tipo_duplicata="nome_duplicado",
        acao="versao_numerada_criada",
        nome_original=final_name,
        nome_versionado=destination_path.name,
        hash_novo=file_hash,
        hash_original="diferente"
    )
    LOGGER.info(f"üìù Nome duplicado: {final_name} ‚Üí {destination_path.name}")
```

---

## üîÑ Fluxo de Processamento Atualizado

### 1Ô∏è‚É£ **Arquivo Novo (Primeira Vez)**

```
1. Calcular hash SHA-256
2. Verificar se hash existe no cache ‚Üí N√ÉO
3. Processar documento (extra√ß√£o + LLM)
4. Gerar nome final
5. Verificar se nome j√° existe ‚Üí N√ÉO
6. Copiar/mover arquivo
7. REGISTRAR hash no cache
8. Salvar cache em disco
```

### 2Ô∏è‚É£ **Arquivo Duplicado por Hash (Conte√∫do Id√™ntico)**

```
1. Calcular hash SHA-256
2. Verificar se hash existe no cache ‚Üí SIM ‚úÖ
3. Buscar registro existente
4. Gerar log audit√°vel (tipo: hash_identico)
5. PULAR processamento (economiza chamada LLM)
6. Incrementar contador de duplicatas
```

**Log Gerado**:
```
INFO: ‚è≠Ô∏è  Arquivo duplicado detectado (hash: abc123def456...) - pulando processamento
INFO: DUPLICATA: {"evento":"duplicata_detectada","tipo_duplicata":"hash_identico",...}
```

### 3Ô∏è‚É£ **Arquivo com Nome Duplicado (Conte√∫do Diferente)**

```
1. Calcular hash SHA-256
2. Verificar se hash existe no cache ‚Üí N√ÉO
3. Processar documento (extra√ß√£o + LLM)
4. Gerar nome final: "2025-10-17-joao-exame.pdf"
5. Verificar se nome j√° existe ‚Üí SIM ‚úÖ
6. Gerar vers√£o numerada: "2025-10-17-joao-exame_v2.pdf"
7. Gerar log audit√°vel (tipo: nome_duplicado)
8. Copiar/mover arquivo com novo nome
9. REGISTRAR hash no cache
```

**Log Gerado**:
```
INFO: üìù Nome duplicado: 2025-10-17-joao-exame.pdf ‚Üí 2025-10-17-joao-exame_v2.pdf
INFO: DUPLICATA: {"evento":"duplicata_detectada","tipo_duplicata":"nome_duplicado",...}
```

### 4Ô∏è‚É£ **Reprocessamento For√ßado (`--force-reprocess`)**

```
1. Flag --force-reprocess ativada
2. IGNORAR verifica√ß√£o de hash no cache
3. Processar documento normalmente (extra√ß√£o + LLM)
4. ATUALIZAR registro no cache (sobrescrever)
```

**Caso de Uso**: Reclassificar documentos ap√≥s melhorias no prompt ou mudan√ßa de modelo LLM.

---

## üìä Estrutura de Dados

### Arquivo: `.clinikondo/processed_hashes.json`

```json
{
  "abc123def456...": {
    "hash_sha256": "abc123def456...",
    "arquivo_original": "/home/user/entrada/exame1.pdf",
    "arquivo_destino": "/home/user/saida/joao_silva/exames/2025-10-17-joao-exame.pdf",
    "timestamp": "2025-10-17T10:30:00.123456",
    "paciente_slug": "joao_silva",
    "tipo_documento": "exame"
  },
  "789xyz...": {
    ...
  }
}
```

---

## üß™ Testes de Valida√ß√£o

### Teste 1: Arquivo Novo
```bash
# Processar arquivo pela primeira vez
python -m src.clinikondo processar --input entrada --output saida --model gpt-4

# Verificar:
# ‚úÖ Arquivo processado
# ‚úÖ Hash registrado em .clinikondo/processed_hashes.json
# ‚úÖ Nenhum log de duplicata
```

### Teste 2: Duplicata por Hash
```bash
# Copiar mesmo arquivo para entrada novamente
cp entrada/exame1.pdf entrada/exame1_copia.pdf

# Processar novamente
python -m src.clinikondo processar --input entrada --output saida --model gpt-4

# Verificar:
# ‚úÖ "Arquivo duplicado detectado" no log
# ‚úÖ Processamento pulado (economiza LLM)
# ‚úÖ Log JSON com tipo_duplicata="hash_identico"
# ‚úÖ Estat√≠sticas mostram "X duplicata(s) ignorada(s)"
```

### Teste 3: Nome Duplicado
```bash
# Editar arquivo (conte√∫do diferente, mas mesmo paciente/tipo/data)
# Processar
python -m src.clinikondo processar --input entrada --output saida --model gpt-4

# Verificar:
# ‚úÖ Arquivo processado normalmente
# ‚úÖ Nome versionado: "arquivo_v2.pdf"
# ‚úÖ Log JSON com tipo_duplicata="nome_duplicado"
```

### Teste 4: Reprocessamento For√ßado
```bash
# Reprocessar arquivo j√° existente
python -m src.clinikondo processar \
  --input entrada \
  --output saida \
  --model gpt-4 \
  --force-reprocess

# Verificar:
# ‚úÖ Arquivo reprocessado (mesmo com hash existente)
# ‚úÖ Chamada LLM executada
# ‚úÖ Cache atualizado
```

---

## üìà Benef√≠cios Implementados

### 1. **Economia de Custos** üí∞
- Evita chamadas LLM desnecess√°rias para arquivos duplicados
- Log mostra "custo_economizado": "1_chamada_llm"

### 2. **Preserva√ß√£o de Dados** üõ°Ô∏è
- Arquivos com mesmo nome mas conte√∫do diferente ‚Üí versionamento autom√°tico
- Nunca sobrescreve arquivos existentes

### 3. **Auditabilidade** üìã
- Todos os logs em formato JSON estruturado
- Rastreamento completo de hashes processados
- Timestamp de cada opera√ß√£o

### 4. **Flexibilidade** üîÑ
- Flag `--force-reprocess` para casos especiais
- Configur√°vel via CLI ou vari√°vel de ambiente

### 5. **Performance** ‚ö°
- Hash calculado apenas uma vez por arquivo
- Cache em mem√≥ria durante execu√ß√£o
- Persist√™ncia apenas ao final

---

## üîê Seguran√ßa

- ‚úÖ Hash SHA-256 para identifica√ß√£o √∫nica e segura
- ‚úÖ Valida√ß√£o de caminhos (preven√ß√£o de path traversal)
- ‚úÖ Logs n√£o exp√µem informa√ß√µes sens√≠veis
- ‚úÖ Arquivo de cache protegido em `.clinikondo/`

---

## üöÄ Pr√≥ximos Passos (Opcional)

1. **Dashboard de Duplicatas**
   - Comando `verificar-duplicatas` j√° existe
   - Pode ser expandido para mostrar estat√≠sticas de cache

2. **Limpeza de Cache**
   - Comando para remover registros antigos
   - Compacta√ß√£o de cache ap√≥s X dias

3. **Relat√≥rio de Economia**
   - Mostrar total de chamadas LLM economizadas
   - Estimativa de custo evitado

---

## üìù Notas de Compatibilidade

- ‚úÖ **Compat√≠vel** com vers√µes anteriores (cache √© criado automaticamente)
- ‚úÖ **Dry-run** n√£o registra hashes (comportamento correto)
- ‚úÖ **Multi-modelo** totalmente suportado
- ‚úÖ **OCR Strategies** (hybrid, multimodal, traditional) funcionam normalmente

---

## üéâ Conclus√£o

Sistema de detec√ß√£o de duplicatas **100% implementado** conforme especifica√ß√µes da **Se√ß√£o 6** do SRS v2.0:

- ‚úÖ Duplicata por Hash ‚Üí Pular com log informativo
- ‚úÖ Nome Duplicado ‚Üí Vers√£o numerada autom√°tica
- ‚úÖ Flag `--force-reprocess` ‚Üí Reprocessamento intencional
- ‚úÖ Logs Audit√°veis ‚Üí JSON estruturado

**Status**: ‚ú® **PRONTO PARA USO** ‚ú®

---

*Documento gerado em 17 de Outubro de 2025*  
*CliniKondo v2.1 - Sistema de Gest√£o de Duplicatas*
